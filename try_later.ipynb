{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d9a5b",
   "metadata": {},
   "source": [
    "### FlexCNN – A Configurable Convolutional Neural Network\n",
    "\n",
    "The FlexCNN class defines a customizable CNN architecture that can be adjusted through hyperparameters such as the number of layers, filter sizes, kernel sizes, dropout rate, and fully connected layer size.\n",
    "\n",
    "The model is composed of two main parts:\n",
    "\n",
    "1.\tFeature extractor – built dynamically in the constructor (__init__), consisting of multiple convolutional blocks. Each block includes a convolutional layer, ReLU activation, and max pooling, allowing the network to learn increasingly abstract spatial features.\n",
    "\n",
    "2.\tClassifier – created automatically during the first forward pass, once the feature map size is known. It includes dropout for regularization, one hidden fully connected layer, and an output layer for classification (e.g., 120 classes).\n",
    "\n",
    "This design allows the CNN to adapt its structure to different configurations without manually redefining the architecture each time, making it ideal for experiments in hyperparameter tuning and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c8b14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66415eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible Convolutional Neural Network with a dynamically created classifier.\n",
    "\n",
    "    This CNN's architecture is defined by the provided hyperparameters,\n",
    "    allowing for a variable number of convolutional layers. The classifier\n",
    "    (fully connected layers) is constructed during the first forward pass\n",
    "    to adapt to the output size of the convolutional feature extractor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_filters, kernel_sizes, dropout_rate, fc_size):\n",
    "        \"\"\"\n",
    "        Initialize the feature extraction portion of the network.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): Number of convolutional blocks to build.\n",
    "            n_filters (list[int]): Output channels for each block.\n",
    "            kernel_sizes (list[int]): Kernel sizes for the convolutions.\n",
    "            dropout_rate (float): Dropout probability used in the classifier.\n",
    "            fc_size (int): Hidden size of the fully connected layer.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(FlexCNN, self).__init__()\n",
    "\n",
    "        # Initialize an empty list to hold the convolutional blocks\n",
    "        blocks = []\n",
    "        # Set the initial number of input channels for RGB images\n",
    "        in_channels = 3\n",
    "\n",
    "        # Loop to construct each convolutional block\n",
    "        for i in range(n_layers):\n",
    "\n",
    "            # Get the parameters for the current convolutional layer\n",
    "            out_channels = n_filters[i]\n",
    "            kernel_size = kernel_sizes[i]\n",
    "            # Calculate padding to maintain the input spatial dimensions ('same' padding)\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "            # Define a block as a sequence of Conv, ReLU, and MaxPool layers\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            \n",
    "            # Add the newly created block to the list\n",
    "            blocks.append(block)\n",
    "\n",
    "            # Update the number of input channels for the next block\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Combine all blocks into a single feature extractor module\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "\n",
    "        # Store hyperparameters needed for building the classifier later\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc_size = fc_size\n",
    "\n",
    "        # The classifier will be initialized dynamically in the forward pass\n",
    "        self.classifier = None\n",
    "\n",
    "    def _create_classifier(self, flattened_size, device):\n",
    "        \"\"\"\n",
    "        Dynamically create and initialize the classifier head.\n",
    "\n",
    "        Args:\n",
    "            flattened_size (int): Number of input features for the first linear layer.\n",
    "            device (torch.device): Device to move the classifier to.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Define the classifier's architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(flattened_size, self.fc_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.fc_size, 120)  # Assumes 120 output classes\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape ``(batch_size, channels, height, width)``.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each dog breed.\n",
    "        \"\"\"\n",
    "        # Get the device of the input tensor to ensure consistency\n",
    "        device = x.device\n",
    "\n",
    "        # Pass the input through the feature extraction layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Flatten the feature map to prepare it for the fully connected layers\n",
    "        flattened = torch.flatten(x, 1)\n",
    "        flattened_size = flattened.size(1)\n",
    "\n",
    "        # If the classifier has not been created yet, initialize it\n",
    "        if self.classifier is None:\n",
    "            self._create_classifier(flattened_size, device)\n",
    "\n",
    "        # Pass the flattened features through the classifier to get the final output\n",
    "        return self.classifier(flattened)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53a661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3b9b75",
   "metadata": {},
   "source": [
    "### Optuna Objective Function\n",
    "\n",
    "The objective() function defines the core of the Optuna optimization process.\n",
    "For each trial, Optuna samples a new combination of hyperparameters, builds and trains a CNN model using those values, and then evaluates its performance on the validation set.\n",
    "\n",
    "Specifically, this function:\n",
    "\n",
    "•\tRandomly selects key hyperparameters (e.g., number of layers, filters, kernel sizes, dropout rate, and fully connected layer size).  \n",
    "•\tConstructs a FlexibleCNN model with the sampled configuration and trains it for a fixed number of epochs using the Adam optimizer and cross-entropy loss.  \n",
    "•\tEvaluates the model’s validation accuracy after training and returns it to Optuna.\n",
    "\n",
    "Optuna then uses this returned accuracy to guide its search, iteratively refining hyperparameter combinations to find the configuration that yields the highest validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209d909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee57c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, device):\n",
    "    \"\"\"\n",
    "    Run a single Optuna trial that trains ``FlexCNN`` and reports validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Current trial used to suggest hyperparameters.\n",
    "        device (torch.device): Device on which the model and tensors should live.\n",
    "\n",
    "    Returns:\n",
    "        float: Final validation accuracy achieved by the sampled configuration.\n",
    "    \"\"\"\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    n_filters = [\n",
    "        trial.suggest_int(f'n_filters_{i}', 16, 128)\n",
    "        for i in range(n_layers)\n",
    "    ]\n",
    "    kernel_sizes = [\n",
    "        trial.suggest_categorical(f'kernel_size_{i}', [3, 5])\n",
    "        for i in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    fc_size = trial.suggest_int('fc_size', 64, 256)\n",
    "\n",
    "    model = FlexCNN(\n",
    "        n_layers=n_layers,\n",
    "        n_filters=n_filters,\n",
    "        kernel_sizes=kernel_sizes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        fc_size=fc_size,\n",
    "    ).to(device)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    writer = SummaryWriter(log_dir=f\"runs/optuna/trial_{trial.number}\")\n",
    "\n",
    "    train_loader = trainloader_with_aug\n",
    "    val_loader = validationloader\n",
    "\n",
    "    n_epochs = 10\n",
    "    helper_utils.train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataloader=train_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "    )\n",
    "\n",
    "    _, accuracy = helper_utils.validate_epoch(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        loss_fcn=loss_fcn,\n",
    "        device=device,\n",
    "    )\n",
    "    writer.add_hparams(\n",
    "        {\n",
    "            \"n_layers\": n_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"fc_size\": fc_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            **{f\"n_filters_{i}\": n_filters[i] if i < len(n_filters) else 0 for i in range(3)},\n",
    "            **{f\"kernel_size_{i}\": kernel_sizes[i] if i < len(kernel_sizes) else 0 for i in range(3)},\n",
    "        },\n",
    "        {\n",
    "            \"val_accuracy\": accuracy,\n",
    "        },\n",
    "    )\n",
    "    writer.close()\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55252558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a study object and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# # Start the optimization process\n",
    "# n_trials = 20\n",
    "# study.optimize(lambda trial: objective(trial, device), n_trials=n_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
